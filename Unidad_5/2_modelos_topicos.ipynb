{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a6a64b8",
      "metadata": {
        "id": "9a6a64b8"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1WNLKH10YpQNNk9eeRIyYLwGkxNbNp-Mm\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db647478",
      "metadata": {
        "id": "db647478"
      },
      "source": [
        "# Modelos de Tópicos\n",
        "---\n",
        "\n",
        "En este notebook veremos una introducción a los modelos de tópicos desde _Python_. Comenzamos importando las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0dfb962",
      "metadata": {
        "id": "e0dfb962"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ce5952",
      "metadata": {
        "id": "53ce5952"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from unidecode import unidecode\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c992ed4",
      "metadata": {
        "id": "8c992ed4"
      },
      "source": [
        "## **1. Motivación**\n",
        "---\n",
        "\n",
        "Un modelo de tópicos es una técnica de procesamiento del lenguaje natural que se utiliza para analizar y entender el contenido de un corpus. El objetivo es identificar los temas o temas subyacentes que se discuten en esos documentos. Es una forma de agrupar y resumir automáticamente información contenida en un gran número de textos. Por ejemplo, en la siguiente figura vemos un ejemplo aplicado sobre un documento científico relacionado con bioinformática. Podemos ver que se encuentran algunos tópicos que podemos relacionar con genética (tópico 1), biología (tópico 2), computación (tópico 3). De la misma forma, también podemos ver qué tan importante es cada tópico para el documento en específico. Más adelante veremos cómo podemos interpretar estos modelos.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=11ARBlZx2qpXjqmMzpohe28iOLD-M2MFn\" width=\"80%\">\n",
        "\n",
        "Los modelos de tópicos utilizan una variedad de algoritmos para analizar el contenido de los documentos y extraer los temas subyacentes. Una vez que se han identificado los temas, se pueden utilizar para clasificar automáticamente nuevos documentos, hacer recomendaciones, generar resúmenes, entre otras aplicaciones.\n",
        "\n",
        "En la creación de un modelo de tópicos se suelen seguir los siguientes pasos:\n",
        "\n",
        "- **Selección y limpieza de los datos**: se recopilan y se limpian los datos a analizar para quedarnos con solo la información relevante.\n",
        "- **Tokenización del texto**: se divide el texto en palabras o frases para poder analizarlas de manera individual.\n",
        "- **Eliminación de palabras irrelevantes**: se eliminan las palabras que no aportan información relevante para el análisis, como artículos o preposiciones.\n",
        "- **Creación del diccionario**: se crea un diccionario de todas las palabras relevantes y se les asigna un valor numérico.\n",
        "- **Creación del modelo**: se utiliza un algoritmo específico para analizar los datos y asignar un tópico a cada palabra.\n",
        "\n",
        "Existen diferentes algoritmos y técnicas que se utilizan para crear modelos de tópicos, los más populares son: **Latent Semantic Analysis (LSA)** y **Latent Dirichlet Allocation (LDA)**. Veremos cómo podemos entrenar este tipo de modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b45e57c",
      "metadata": {
        "id": "8b45e57c"
      },
      "source": [
        "## **2. Carga de Datos**\n",
        "---\n",
        "\n",
        "Como los modelos de tópicos se entrenan de forma no supervisada, lo único que necesitamos para entrenarlos es un conjunto de textos (también es posible utilizar un único documento si tokenizamos por oraciones).\n",
        "\n",
        "En este caso usaremos el conjunto de datos **20 newsgroups**, la cual trata de una colección de alrededor de 11000 mensajes de noticias en 20 diferentes categorías o grupos de noticias. Los 20 grupos de noticias incluyen temas como ciencias políticas, religión, deportes, tecnología y ciencia. Cada mensaje incluye el encabezado del correo electrónico, el cuerpo del mensaje y el remitente. Los mensajes están en inglés y se encuentran en formato de texto plano.\n",
        "\n",
        "Fue recolectado por el equipo de investigación de aprendizaje automático de la Universidad de Massachusetts en los años 90 y se ha utilizado ampliamente como una de las principales fuentes de datos para evaluar los algoritmos de aprendizaje automático y procesamiento del lenguaje natural. Este conjunto de datos es ampliamente utilizado en la investigación y la educación, ya que proporciona una variedad de textos en diferentes temas y una estructura de datos fácil de trabajar. Se utiliza para evaluar los algoritmos de clasificación de texto, extracción de características, agrupamiento de texto y otros problemas relacionados con el procesamiento del lenguaje natural.\n",
        "\n",
        "Este conjunto de datos lo podemos cargar directamente desde `sklearn`, importamos la función correspondiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec21356",
      "metadata": {
        "id": "fec21356"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a518e6a",
      "metadata": {
        "id": "2a518e6a"
      },
      "source": [
        "Procedemos a cargar el conjunto de datos eliminando algunos artefactos relacionados a etiquetas HTML:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8192481a",
      "metadata": {
        "id": "8192481a"
      },
      "outputs": [],
      "source": [
        "dataset = fetch_20newsgroups(remove=(\"headers\", \"footer\", \"quotes\"))\n",
        "display(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb292fe",
      "metadata": {
        "id": "3bb292fe"
      },
      "source": [
        "Ahora, extraemos el texto y los posibles tipos de noticias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1442ef3",
      "metadata": {
        "id": "b1442ef3"
      },
      "outputs": [],
      "source": [
        "corpus = dataset.data\n",
        "labels = dataset.target_names\n",
        "print(len(corpus))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dda0fb4",
      "metadata": {
        "id": "7dda0fb4"
      },
      "source": [
        "Veamos un documento al azar del conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45de5b8f",
      "metadata": {
        "id": "45de5b8f"
      },
      "outputs": [],
      "source": [
        "idx = np.random.randint(len(corpus))\n",
        "doc = corpus[idx]\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87aa61ab",
      "metadata": {
        "id": "87aa61ab"
      },
      "source": [
        "Ahora, veamos los posibles tipos de noticias en el corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f56846f",
      "metadata": {
        "id": "3f56846f"
      },
      "outputs": [],
      "source": [
        "display(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564e9ea6",
      "metadata": {
        "id": "564e9ea6"
      },
      "source": [
        "Estos corresponden a:\n",
        "\n",
        "- `alt.atheism`: Noticias relacionadas con el ateísmo.\n",
        "- `comp.graphics`: Noticias relacionadas con la computación gráfica.\n",
        "- `comp.os.ms-windows.misc`: Noticias relacionadas con el sistema operativo Microsoft Windows.\n",
        "- `comp.sys.ibm.pc.hardware`: Noticias relacionadas con el hardware de la computadora IBM PC.\n",
        "- `comp.sys.mac.hardware`: Noticias relacionadas con el hardware de la computadora Macintosh.\n",
        "- `comp.windows.x`: Noticias relacionadas con la interfaz gráfica de usuario de Windows X.\n",
        "- `misc.forsale`: Noticias relacionadas con la venta de artículos.\n",
        "- `rec.autos`: Noticias relacionadas con los automóviles.\n",
        "- `rec.motorcycles`: Noticias relacionadas con las motocicletas.\n",
        "- `rec.sport.baseball`: Noticias relacionadas con el béisbol.\n",
        "- `rec.sport.hockey`: Noticias relacionadas con el hockey sobre hielo.\n",
        "- `sci.crypt`: Noticias relacionadas con la criptografía.\n",
        "- `sci.electronics`: Noticias relacionadas con la electrónica.\n",
        "- `sci.med`: Noticias relacionadas con la medicina.\n",
        "- `sci.space`: Noticias relacionadas con el espacio.\n",
        "- `soc.religion.christian`: Noticias relacionadas con la religión cristiana.\n",
        "- `talk.politics.guns`: Noticias relacionadas con la política de armas.\n",
        "- `talk.politics.mideast`: Noticias relacionadas con la política del Medio Oriente.\n",
        "- `talk.politics.misc`: Noticias relacionadas con la política en general.\n",
        "- `talk.religion.misc`: Noticias relacionadas con la religión en general.\n",
        "\n",
        "Es importante tener en cuenta que algunos de estos temas pueden ser amplios y pueden incluir subtemas específicos.\n",
        "\n",
        "Ahora, vamos a definir una función para preprocesar los textos. Primero definimos un pipeline de `spacy` (para tokenizar, eliminar stopwords y etiquetar partes del discurso - POS tagging)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a60968",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "c3a60968"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm',\n",
        "                 exclude=['parser', 'senter', 'lemmatizer', 'ner'])\n",
        "nlp.component_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c5f59f",
      "metadata": {
        "id": "99c5f59f"
      },
      "source": [
        "Ahora, definimos la función de preprocesamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636e12c2",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "636e12c2"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    no_chars = re.sub(r\"[^a-z ]\", \" \", text) # eliminamos caracteres especiales\n",
        "    doc = nlp(no_chars) # creamos un documento de spacy\n",
        "    no_stops = \" \".join(\n",
        "        token.text\n",
        "        for token in filter(\n",
        "            lambda token: not token.is_stop  # eliminamos stopwords\n",
        "                          and len(token) > 3 and len(token) < 24 # eliminamos palabras por longitud\n",
        "                          and token.pos_ in ['NOUN', 'PROPN'], # dejamos solo sustantivos o nombres propios\n",
        "            doc,\n",
        "            )\n",
        "        )\n",
        "    norm_text = unidecode(no_stops.lower()) # normalizamos el texto\n",
        "    no_spaces = re.sub(r\"\\s+\", \" \", norm_text) # eliminamos espacios duplicados\n",
        "    return no_spaces.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa6d4f5b",
      "metadata": {
        "id": "fa6d4f5b"
      },
      "source": [
        "Preprocesamos el corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143c7f69",
      "metadata": {
        "id": "143c7f69"
      },
      "outputs": [],
      "source": [
        "prep_corpus = list(map(preprocess, corpus))\n",
        "display(prep_corpus[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246d825f",
      "metadata": {
        "id": "246d825f"
      },
      "source": [
        "## **3. Extracción de Características**\n",
        "---\n",
        "\n",
        "Los modelos de tópicos por lo general se aplican sobre representaciones basadas en bolsas de palabras. Más adelante hablaremos un poco más en detalle de cada modelo. Por el momento nos concentraremos en entrenar vectorizadores para conteo de palabras y para TF-IDF. Comenzamos importándolos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b0466a",
      "metadata": {
        "id": "02b0466a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19823be4",
      "metadata": {
        "id": "19823be4"
      },
      "source": [
        "Entrenamos los vectorizadores para utilizar únicamente los 500 términos más frecuentes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9689c65b",
      "metadata": {
        "id": "9689c65b"
      },
      "outputs": [],
      "source": [
        "bow = CountVectorizer(max_features=500).fit(prep_corpus)\n",
        "tfidf = TfidfVectorizer(max_features=500).fit(prep_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2320dc0",
      "metadata": {
        "id": "b2320dc0"
      },
      "source": [
        "Ahora, extraemos las representaciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748d9f07",
      "metadata": {
        "id": "748d9f07"
      },
      "outputs": [],
      "source": [
        "features_bow = bow.transform(corpus)\n",
        "features_tfidf = tfidf.transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4884d83b",
      "metadata": {
        "id": "4884d83b"
      },
      "source": [
        "También extraemos el vocabulario de cada tokenizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9fc863",
      "metadata": {
        "id": "aa9fc863"
      },
      "outputs": [],
      "source": [
        "vocab_bow = bow.get_feature_names_out()\n",
        "vocab_tfidf = tfidf.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d24bfa2d",
      "metadata": {
        "id": "d24bfa2d"
      },
      "source": [
        "## **4. Latent Semantic Analysis**\n",
        "---\n",
        "\n",
        "El Análisis Semántico Latente (Latent Semantic Analysis o LSA) es una técnica de procesamiento del lenguaje natural que se utiliza para analizar y comprender el contenido de un conjunto de documentos. El objetivo es identificar los temas subyacentes que se discuten en esos documentos y establecer relaciones semánticas entre las palabras y frases. Es una forma de agrupar y resumir automáticamente la información contenida en un gran número de textos.\n",
        "\n",
        "LSA se basa en la idea de que las palabras que aparecen juntas en un documento tienden a tener un significado relacionado. Por ejemplo, las palabras \"perro\" y \"ladrar\" probablemente están relacionadas semánticamente, ya que un perro ladra. El objetivo de LSA es encontrar estas relaciones semánticas ocultas entre las palabras en un conjunto de documentos.\n",
        "\n",
        "Para llevar a cabo el análisis, se crea una matriz documento-término (bolsa de palabras) que representa los conteos de los términos (palabras) en los documentos. Cada celda en la matriz contiene el número de veces que aparece un término en un documento. Esta matriz se llama matriz de contenido. A continuación, se aplica una técnica de reducción de dimensionalidad, como la descomposición en valores singulares (SVD), para reducir la dimensionalidad de la matriz de contenido.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1f-GEwcEPXMFLETqhq2QRVEZg_whsGZAb\" width=\"80%\">\n",
        "\n",
        "SVD divide la matriz en tres componentes: una matriz documento-tópico $\\mathbf{U}$, una matriz tópico-término $\\mathbf{V}$ y una matriz de importancias $\\mathbf{S}$. Estas tres matrices tienen su interpretación:\n",
        "\n",
        "- **Matriz documento-tópico**: se trata de una matriz de características (funciona como _embedding_) que representa cada documento como una composición de varios tópicos.\n",
        "- **Matriz de importancias**: esta matriz muestra qué tan importante es cada tópico para todo el corpus.\n",
        "- **Matriz tópico-término**: se trata de una matriz que muestra qué tan importante es cada término para un tema."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d96562",
      "metadata": {
        "id": "d2d96562"
      },
      "source": [
        "### **4.1. Implementación**\n",
        "---\n",
        "\n",
        "Para implementar este modelo vamos a usar la descomposición de `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68502864",
      "metadata": {
        "id": "68502864"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae8b435",
      "metadata": {
        "id": "5ae8b435"
      },
      "source": [
        "En **LSA** se utiliza el enfoque **TF-IDF** en lugar de contar simplemente el número de veces que aparece un término en un documento, por varias razones:\n",
        "\n",
        "- **Manejo de términos irrelevantes**: algunos términos comunes aparecen con mucha frecuencia en todos los documentos y no aportan información relevante para el análisis. TF-IDF tiene en cuenta estos términos y los penaliza.\n",
        "- **Importancia relativa de los términos**: algunos términos son más importantes que otros en un documento. TF-IDF tiene en cuenta no solo la frecuencia de un término en un documento, sino también su frecuencia en todos los documentos del corpus.\n",
        "- **Discriminación de términos**: LSA utiliza SVD para reducir la dimensionalidad de la matriz de representación, y algunos términos son necesarios para discriminar entre los documentos. TF-IDF ayuda a identificar estos términos importantes y a incluirlos en la representación.\n",
        "- **Mejora de la precisión**: al utilizar TF-IDF en lugar de conteos simples, se obtiene una representación continua con una magnitud controlada, esto facilita la precisión numérica en el algoritmo SVD.\n",
        "\n",
        "Los hiperparámetros de TruncatedSVD de `sklearn` para implementar LSA son:\n",
        "\n",
        "- `n_components`: Es el número de componentes o dimensiones deseadas en la reducción de dimensionalidad. Este valor representa el número de tópicos o temas que deseamos encontrar.\n",
        "- `algorithm`: El algoritmo utilizado para calcular la descomposición SVD. Los valores posibles son \"arpack\" y \"randomized\". El valor predeterminado es \"randomized\" que es más rápido para matrices grandes.\n",
        "- `n_iter`: Número de iteraciones para el algoritmo \"randomized\". El valor predeterminado es 5.\n",
        "- `random_state`: Es la semilla utilizada para la inicialización del generador de números aleatorios.\n",
        "\n",
        "Es importante tener en cuenta que, al elegir los valores para estos hiperparámetros, debe tener en cuenta el tamaño de su conjunto de datos y los requisitos de precisión y tiempo de ejecución. En este caso usaremos los siguientes valores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7e5f34",
      "metadata": {
        "id": "2c7e5f34"
      },
      "outputs": [],
      "source": [
        "lsa = TruncatedSVD(\n",
        "        n_components = 20,\n",
        "        algorithm = \"randomized\",\n",
        "        random_state = 42,\n",
        "        n_iter = 100\n",
        "        ).fit(features_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "431b7796",
      "metadata": {
        "id": "431b7796"
      },
      "source": [
        "### **4.2. Tópicos por Documento**\n",
        "---\n",
        "\n",
        "Comenzaremos extrayendo una matriz de representación de los textos como si fuera un _embedding_, para esto, usaremos el método `transform` del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15be6aeb",
      "metadata": {
        "id": "15be6aeb"
      },
      "outputs": [],
      "source": [
        "features_lsa = lsa.transform(features_tfidf)\n",
        "print(features_lsa.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721b2c5c",
      "metadata": {
        "id": "721b2c5c"
      },
      "source": [
        "Como podemos ver, cada documento fue mapeado a una representación de tamaño 20. No obstante, se trata de una representación que resume todo el vocabulario en únicamente 20 temas.\n",
        "\n",
        "Podemos visualizar la representación de un documento específico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f4bd55a",
      "metadata": {
        "id": "6f4bd55a"
      },
      "outputs": [],
      "source": [
        "doc_id = 0\n",
        "doc_features = features_lsa[doc_id]\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(np.arange(doc_features.size), doc_features);\n",
        "ax.set_xlabel(\"Tópico\");\n",
        "ax.set_ylabel(\"Valor\");\n",
        "ax.set_xticks(np.arange(doc_features.size));\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "274d6364",
      "metadata": {
        "id": "274d6364"
      },
      "source": [
        "LSA se caracteriza por ser el modelo de tópicos de menor costo computacional (se entrena bastante rápido). No obstante, no se puede interpretar tan fácilmente, ya que utiliza valores negativos y en una escala que depende de los datos originales.\n",
        "\n",
        "En ese caso, podemos ver la importancia de cada tópico en un documento específico si miramos la magnitud de su representación, es decir, el valor absoluto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8299b954",
      "metadata": {
        "id": "8299b954"
      },
      "outputs": [],
      "source": [
        "doc_id = 0\n",
        "doc_features = np.abs(features_lsa[doc_id])\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(np.arange(doc_features.size), doc_features)\n",
        "ax.set_xlabel(\"Tópico\")\n",
        "ax.set_ylabel(\"Valor\")\n",
        "ax.set_xticks(np.arange(doc_features.size));\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540bf429",
      "metadata": {
        "id": "540bf429"
      },
      "source": [
        "Esta representación suele ser muy usada como _embedding_ de documentos. Veamos un ejemplo de similitud semántica con el siguiente documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323f1711",
      "metadata": {
        "id": "323f1711"
      },
      "outputs": [],
      "source": [
        "doc_id = 1\n",
        "print(corpus[doc_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72e7c92b",
      "metadata": {
        "id": "72e7c92b"
      },
      "source": [
        "Como podemos ver, es un texto que habla de hardware de computadores. Vamos a evaluar la similitud coseno con la representación LSA de cada uno de los documentos para determinar los más similares. Importamos la función para la similitud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a291fc6a",
      "metadata": {
        "id": "a291fc6a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c022b2a5",
      "metadata": {
        "id": "c022b2a5"
      },
      "source": [
        "Evaluamos la similitud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37eb6623",
      "metadata": {
        "id": "37eb6623"
      },
      "outputs": [],
      "source": [
        "sim = cosine_similarity(features_lsa[doc_id, np.newaxis], features_lsa).flatten()\n",
        "print(sim.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39c0ed63",
      "metadata": {
        "id": "39c0ed63"
      },
      "source": [
        "Ahora, creamos un `DataFrame` para encontrar el top de los documentos más similares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c0a064",
      "metadata": {
        "id": "48c0a064"
      },
      "outputs": [],
      "source": [
        "sims = pd.DataFrame(data={\"text\": corpus, \"sim\": sim})\n",
        "top5 = (\n",
        "        sims\n",
        "        .sort_values(by=\"sim\", ascending=False)\n",
        "        .head(6)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4791687a",
      "metadata": {
        "id": "4791687a"
      },
      "source": [
        "Veamos los documentos más relevantes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5051573",
      "metadata": {
        "id": "d5051573"
      },
      "outputs": [],
      "source": [
        "for doc in top5.iloc[1:, 0]:\n",
        "    print(\"=\" * 50)\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5372ecdc",
      "metadata": {
        "id": "5372ecdc"
      },
      "source": [
        "Como podemos ver, estos documentos también hablan sobre hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "addc919f",
      "metadata": {
        "id": "addc919f"
      },
      "source": [
        "### **4.3. Términos por Tópico**\n",
        "---\n",
        "\n",
        "La matriz tópico-término nos ofrece información muy valiosa para la interpretación de los tópicos. En especial, nos permite saber cuáles son las palabras más importantes en los temas que encontró el modelo.\n",
        "\n",
        "Podemos acceder a esta matriz por medio del atributo `components_` del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4618427a",
      "metadata": {
        "id": "4618427a"
      },
      "outputs": [],
      "source": [
        "components = lsa.components_\n",
        "print(components.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba5373b",
      "metadata": {
        "id": "8ba5373b"
      },
      "source": [
        "Como podemos ver, nos muestra la importancia de cada una de las 500 palabras del vocabulario para cada uno de los 20 tópicos. Veamos el top 15 de las palabras más importantes por cada tópico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6188df8",
      "metadata": {
        "id": "d6188df8"
      },
      "outputs": [],
      "source": [
        "# Iteramos sobre cada tópico\n",
        "for i, comp in enumerate(components):\n",
        "    # Juntamos los términos con cada uno de los valores en la matriz V\n",
        "    terms_comp = zip(vocab_tfidf, np.abs(comp))\n",
        "    # Ordenamos los términos de acuerdo al resultado de LSA\n",
        "    sorted_terms = sorted(\n",
        "            terms_comp,\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "            )[:15]\n",
        "    # Mostramos los términos más importantes en cada tópico\n",
        "    print(\n",
        "            \"Tópico {}: {}\".format(\n",
        "                i,\n",
        "                \" \".join(list(map(lambda x:x[0], sorted_terms)))\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab8f987",
      "metadata": {
        "id": "1ab8f987"
      },
      "source": [
        "Con esto podemos ver algunos tópicos clave como:\n",
        "\n",
        "- **Tópico 4**: deportes.\n",
        "- **Tópico 7**: hardware.\n",
        "\n",
        "No obstante, muchos de los otros tópicos parecen mezclar distintos temas (baja coherencia).\n",
        "\n",
        "Este tipo de análisis se puede ver de mejor forma con una visualización de nubes de palabras. Importamos la librería:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc46053",
      "metadata": {
        "id": "1dc46053"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a928725",
      "metadata": {
        "id": "2a928725"
      },
      "source": [
        "Vamos a generar una nube de palabras ponderada con la importancia de cada término en un tópico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a9dbfe3",
      "metadata": {
        "id": "0a9dbfe3"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(4, 5, figsize=(15, 7))\n",
        "cont = 0\n",
        "for i in range(4):\n",
        "    for j in range(5):\n",
        "        ax = axes[i, j]\n",
        "        freqs = {\n",
        "            term: abs(float(importance))\n",
        "            for term, importance in zip(vocab_tfidf, components[cont])\n",
        "        }\n",
        "        wc = WordCloud(background_color=\"white\").generate_from_frequencies(freqs)\n",
        "        ax.imshow(wc)\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(f\"Tópico {cont}\")\n",
        "        cont += 1\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e6a925",
      "metadata": {
        "id": "f9e6a925"
      },
      "source": [
        "### **4.4. Importancia de Tópicos**\n",
        "---\n",
        "\n",
        "También podemos ver qué tan importante es cada tópico dentro del corpus con la matriz de importancias de tópico. Se trata de una matriz diagonal con un valor que puede estar normalizado; se puede extraer por medio del atributo `explained_variance_ratio_` del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ffa8c3",
      "metadata": {
        "id": "84ffa8c3"
      },
      "outputs": [],
      "source": [
        "topic_importances = lsa.explained_variance_ratio_\n",
        "print(topic_importances.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d7c478",
      "metadata": {
        "id": "55d7c478"
      },
      "source": [
        "El resultado es directamente la diagonal de la matriz. Podemos generar una gráfica en forma de diagrama de barras para ver estas importancias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e47f75",
      "metadata": {
        "id": "12e47f75"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.bar(np.arange(topic_importances.size), topic_importances)\n",
        "ax.set_xticks(np.arange(topic_importances.size))\n",
        "ax.set_xlabel(\"Tópico\")\n",
        "ax.set_ylabel(\"Importancia\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3328e551",
      "metadata": {
        "id": "3328e551"
      },
      "source": [
        "## **5. Latent Dirichlet Allocation**\n",
        "---\n",
        "\n",
        "El modelo de tópicos Latent Dirichlet Allocation (LDA) es un algoritmo de aprendizaje automático no supervisado utilizado para identificar los temas subyacentes que se discuten en un conjunto de documentos. LDA asume que cada documento está compuesto por una mezcla de tópicos y que cada tópico está compuesto por una mezcla de palabras.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j1neXJH2PhQiS5Nz1Mi3F8-BY312BvWg\" width=\"80%\">\n",
        "\n",
        "LDA utiliza una distribución de probabilidad generativa para modelar cómo se generan los documentos. La idea es que, dado un conjunto de tópicos, se asigna un tópico a cada palabra en un documento de forma aleatoria. Luego, se utiliza la distribución de probabilidad para generar nuevos documentos que se ajusten al conjunto de datos original.\n",
        "\n",
        "La implementación de LDA requiere especificar el número de tópicos a generar, el número de palabras en cada tópico y las distribuciones de probabilidad para cada uno. Luego, se utiliza un algoritmo de optimización para ajustar estos parámetros al conjunto de datos.\n",
        "\n",
        "LDA es un modelo probabilístico que se basa en la teoría Bayesiana, pero más allá de entender los detalles matemáticos, en este caso nos interesa ver su aplicabilidad práctica. En LDA tenemos los dos siguientes parámetros:\n",
        "\n",
        "- $\\alpha$: es un parámetro de la distribución Dirichlet que representa la densidad documento-tópico. Entre más grande sea el valor de $\\alpha$, cada documento estará conformado por más tópicos.\n",
        "- $\\beta$: es el parámetro _a priori_ que representa la densidad tópico-término. Con un valor grande de $\\beta$ los tópicos se conforman de un mayor número de palabras.\n",
        "\n",
        "LDA es un modelo muy similar a LSA, con la diferencia de que todas las representaciones y elementos internos son probabilidades, lo que facilita su interpretación. No obstante, LDA es un modelo con mayor costo computacional (puede tardar más tiempo en entrenar)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78fd8d6",
      "metadata": {
        "id": "b78fd8d6"
      },
      "source": [
        "### **5.1. Implementación**\n",
        "---\n",
        "\n",
        "Para implementar LDA utilizaremos el modelo de descomposición desde `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf91a81",
      "metadata": {
        "id": "caf91a81"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e1be32",
      "metadata": {
        "id": "36e1be32"
      },
      "source": [
        "En **LDA** se utilizan **conteos de palabras** en lugar de **TF-IDF**, debido a que el modelo de tópicos LDA es una distribución generativa, donde se asume que cada documento es una mezcla de tópicos y cada tópico es una mezcla de palabras.\n",
        "\n",
        "Los conteos de palabras representan la frecuencia de una palabra en un documento, lo que es importante para LDA, ya que asume que las palabras con una mayor frecuencia en un documento son más probables de ser asociadas con el tópico del documento. En cambio, TF-IDF se utiliza para representar la importancia relativa de una palabra en un documento en comparación con el conjunto de documentos, lo que no es necesario para LDA, ya que se asume que cada documento es independiente.\n",
        "\n",
        "Además, LDA se basa en la idea de que los tópicos son distribuciones de probabilidad sobre las palabras, y los conteos de palabras son adecuados para representar esta distribución. Al utilizar conteos de palabras en lugar de TF-IDF, se pueden calcular las probabilidades de las palabras en cada tópico y en cada documento, lo que es esencial para el funcionamiento del modelo.\n",
        "\n",
        "Procedemos a entrenar el modelo con la representación de bolsa de palabras. Tiene los siguientes hiperparámetros:\n",
        "\n",
        "- `n_components`: número de tópicos.\n",
        "- `doc_topic_prior`: valor $\\alpha$.\n",
        "- `topic_word_prior`: valor $\\beta$.\n",
        "- `random_state`: semilla de números aleatorios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26fca1c6",
      "metadata": {
        "id": "26fca1c6"
      },
      "outputs": [],
      "source": [
        "lda = LatentDirichletAllocation(\n",
        "    n_components=20,\n",
        "    doc_topic_prior=1 / 20,\n",
        "    topic_word_prior=1 / 20,\n",
        "    random_state=42,\n",
        "    ).fit(features_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62276600",
      "metadata": {
        "id": "62276600"
      },
      "source": [
        "### **5.2. Tópicos por Documento**\n",
        "---\n",
        "\n",
        "Comenzamos extrayendo la matriz documento-tópico con el método `transform` del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fbd9679",
      "metadata": {
        "id": "0fbd9679"
      },
      "outputs": [],
      "source": [
        "features_lda = lda.transform(features_bow)\n",
        "print(features_lda.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070d7ac8",
      "metadata": {
        "id": "070d7ac8"
      },
      "source": [
        "Podemos visualizar la representación de un documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4799f4b3",
      "metadata": {
        "id": "4799f4b3"
      },
      "outputs": [],
      "source": [
        "doc_id = 0\n",
        "doc_features = features_lda[doc_id]\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(np.arange(doc_features.size), doc_features);\n",
        "ax.set_xlabel(\"Tópico\");\n",
        "ax.set_ylabel(\"Valor\");\n",
        "ax.set_xticks(np.arange(doc_features.size));\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b41150c0",
      "metadata": {
        "id": "b41150c0"
      },
      "source": [
        "Como podemos ver, se trata de valores entre 0 y 1 (son probabilidades) y la suma es 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2256447",
      "metadata": {
        "id": "a2256447"
      },
      "outputs": [],
      "source": [
        "print(np.round(doc_features.sum(),4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572dbcba",
      "metadata": {
        "id": "572dbcba"
      },
      "source": [
        "Al igual que en LSA, el modelo LDA también captura relaciones semánticas entre documentos. Veamos un ejemplo con la similitud coseno, primero seleccionamos un documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b3eb09",
      "metadata": {
        "id": "b4b3eb09"
      },
      "outputs": [],
      "source": [
        "doc_id = 1\n",
        "print(corpus[doc_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f3b564",
      "metadata": {
        "id": "25f3b564"
      },
      "source": [
        "Calculamos la similitud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591c63df",
      "metadata": {
        "id": "591c63df"
      },
      "outputs": [],
      "source": [
        "sim = cosine_similarity(features_lda[doc_id, np.newaxis], features_lda).flatten()\n",
        "print(sim.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39fb5cef",
      "metadata": {
        "id": "39fb5cef"
      },
      "source": [
        "Ahora, creamos un `DataFrame` para encontrar el top de los documentos más similares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd6b6f8",
      "metadata": {
        "id": "afd6b6f8"
      },
      "outputs": [],
      "source": [
        "sims = pd.DataFrame(data={\"text\": corpus, \"sim\": sim})\n",
        "top5 = (\n",
        "        sims\n",
        "        .sort_values(by=\"sim\", ascending=False)\n",
        "        .head(6)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f184a70",
      "metadata": {
        "id": "9f184a70"
      },
      "source": [
        "Veamos los documentos más relevantes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112be842",
      "metadata": {
        "id": "112be842"
      },
      "outputs": [],
      "source": [
        "for doc in top5.iloc[1:, 0]:\n",
        "    print(\"=\" * 50)\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28918ec5",
      "metadata": {
        "id": "28918ec5"
      },
      "source": [
        "Como podemos ver, estos documentos también hablan sobre hardware de computadores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c33f0c6",
      "metadata": {
        "id": "5c33f0c6"
      },
      "source": [
        "### **5.3. Términos por Tópico**\n",
        "---\n",
        "\n",
        "LDA se caracteriza por obtener mejores resultados en comparación con LSA. Esto se puede evidenciar en la matriz tópico-término aprendida, la cual podemos extraer con el atributo `components_` del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "971d58ed",
      "metadata": {
        "id": "971d58ed"
      },
      "outputs": [],
      "source": [
        "components = lda.components_\n",
        "print(components.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "657cebaa",
      "metadata": {
        "id": "657cebaa"
      },
      "source": [
        "Como podemos ver, nos muestra la importancia de cada una de las 500 palabras del vocabulario para cada uno de los 20 tópicos. Veamos el top 15 de las palabras más importantes por cada tópico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f248ff96",
      "metadata": {
        "id": "f248ff96"
      },
      "outputs": [],
      "source": [
        "# Iteramos sobre cada tópico\n",
        "for i, comp in enumerate(components):\n",
        "    # Juntamos los términos con cada uno de los valores en la matriz V\n",
        "    terms_comp = zip(vocab_bow, np.abs(comp))\n",
        "    # Ordenamos los términos de acuerdo al resultado de LSA\n",
        "    sorted_terms = sorted(\n",
        "            terms_comp,\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "            )[:15]\n",
        "    # Mostramos los términos más importantes en cada tópico\n",
        "    print(\n",
        "            \"Tópico {}: {}\".format(\n",
        "                i,\n",
        "                \" \".join(list(map(lambda x:x[0], sorted_terms)))\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c17a7093",
      "metadata": {
        "id": "c17a7093"
      },
      "source": [
        "Con esto podemos ver más tópicos clave como:\n",
        "\n",
        "- **Tópico 3 y 15**: religión.\n",
        "- **Tópico 4**: software.\n",
        "- **Tópico 11 y 17**: hardware.\n",
        "- **Tópico 12 y 16**: ciencia e investigación.\n",
        "- **Tópico 13**: criptografía.\n",
        "- **Tópico 14**: política.\n",
        "- **Tópico 18**: deportes.\n",
        "\n",
        "Vamos a generar una nube de palabras ponderada con la importancia de cada término en un tópico para visualizar mejor los tópicos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4428c6",
      "metadata": {
        "id": "7f4428c6"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(4, 5, figsize=(15, 7))\n",
        "cont = 0\n",
        "for i in range(4):\n",
        "    for j in range(5):\n",
        "        ax = axes[i, j]\n",
        "        freqs = {\n",
        "            term: abs(float(importance))\n",
        "            for term, importance in zip(vocab_bow, components[cont])\n",
        "        }\n",
        "        wc = WordCloud(background_color=\"white\").generate_from_frequencies(freqs)\n",
        "        ax.imshow(wc)\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(f\"Tópico {cont}\")\n",
        "        cont += 1\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81949e54",
      "metadata": {
        "id": "81949e54"
      },
      "source": [
        "### **5.4. Importancia por Tópico**\n",
        "---\n",
        "\n",
        "El modelo LDA no nos da directamente las importancias de cada tópico dentro del corpus. No obstante, como estamos manejando probabilidades, estas importancias se pueden calcular directamente con la matriz de documento-tópico al promediar la contribución de los documentos. Es decir:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3693440e",
      "metadata": {
        "id": "3693440e"
      },
      "outputs": [],
      "source": [
        "topic_importances = features_lda.mean(axis=0)\n",
        "print(topic_importances.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23805a18",
      "metadata": {
        "id": "23805a18"
      },
      "source": [
        "Podemos generar una gráfica en forma de diagrama de barras para ver estas importancias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b9568c",
      "metadata": {
        "id": "11b9568c"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.bar(np.arange(topic_importances.size), topic_importances)\n",
        "ax.set_xticks(np.arange(topic_importances.size));\n",
        "ax.set_xlabel(\"Tópico\")\n",
        "ax.set_ylabel(\"Importancia\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac2238d0",
      "metadata": {
        "id": "ac2238d0"
      },
      "source": [
        "Por último, existe una herramienta conocida como **pyLDAvis**, la cual es una librería de Python para visualizar modelos de tópicos Latent Dirichlet Allocation (LDA) desarrollada por Carson Sievert. La biblioteca proporciona una interfaz web interactiva que permite explorar y comprender fácilmente los tópicos generados por un modelo LDA.\n",
        "\n",
        "pyLDAvis genera una visualización interactiva que muestra los tópicos en un plano de coordenadas de dos dimensiones, donde cada punto representa un tópico y los puntos cercanos entre sí representan tópicos similares. Los tópicos se pueden explorar haciendo clic en ellos para ver las palabras más importantes y los documentos asociados.\n",
        "\n",
        "También incluye un mapa de calor que muestra las palabras más relevantes para cada tópico, una nube de palabras que muestra la frecuencia de las palabras en los tópicos y un gráfico de barras que muestra la distribución de los tópicos en los documentos.\n",
        "\n",
        "pyLDAvis es una herramienta útil para explorar y comprender los resultados de un modelo LDA, y puede ser utilizado para ajustar los parámetros del modelo y para seleccionar el número óptimo de tópicos.\n",
        "\n",
        "Vamos a instalarla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d26c1791",
      "metadata": {
        "id": "d26c1791"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por problemas de compatibilidad, es necesario utilizar una version de `pandas < 2.0.0`. **No debe reiniciar el entorno de ejecución**"
      ],
      "metadata": {
        "id": "b5k2xYBe60Cx"
      },
      "id": "b5k2xYBe60Cx"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.5.3"
      ],
      "metadata": {
        "id": "fx6ak8sv608-"
      },
      "id": "fx6ak8sv608-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c5097547",
      "metadata": {
        "id": "c5097547"
      },
      "source": [
        "Importamos la librería para su uso con `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569865b0",
      "metadata": {
        "id": "569865b0"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model as sklearn_lda"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486d6f12",
      "metadata": {
        "id": "486d6f12"
      },
      "source": [
        "Habilitamos el uso de la librería en notebooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ad6731",
      "metadata": {
        "id": "86ad6731"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c261d8c9",
      "metadata": {
        "id": "c261d8c9"
      },
      "source": [
        "Especificamos a la librería el vectorizador usado y el modelo LDA entrenado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3aac9b",
      "metadata": {
        "id": "ac3aac9b"
      },
      "outputs": [],
      "source": [
        "bow.get_feature_names = bow.get_feature_names_out\n",
        "ldavis_prepared = sklearn_lda.prepare(lda, features_bow, bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe6f1ca",
      "metadata": {
        "id": "efe6f1ca"
      },
      "source": [
        "Podemos visualizar el tablero interactivo de LDA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2453907",
      "metadata": {
        "id": "d2453907"
      },
      "outputs": [],
      "source": [
        "display(ldavis_prepared)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6f9982",
      "metadata": {
        "id": "1c6f9982"
      },
      "source": [
        "También podemos exportarlo como un HTML para embeberlo, reutilizarlo o modificarlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc04869",
      "metadata": {
        "id": "bcc04869"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.save_html(ldavis_prepared, \"./ldavis_prepared.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f9ce35",
      "metadata": {
        "id": "00f9ce35"
      },
      "source": [
        "## **Recursos Adicionales**\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información muy útil para profundizar en los temas vistos en este taller guiado:\n",
        "\n",
        "- [Dimensionality reduction using truncated SVD (aka LSA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
        "- [Latent Dirichlet Allocation](https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d333c59",
      "metadata": {
        "id": "0d333c59"
      },
      "source": [
        "## **Créditos**\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}